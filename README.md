<div align="center">

# ğŸ›¡ï¸ Vision Mamba Adversarial Robustness

### Enhancing Vision Mamba Robustness Against Adversarial Attacks for Indonesian Traffic Sign Recognition

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)]()

<br/>

**Master's Thesis Research Artifact â€” Institut Teknologi Bandung**

[ğŸ“„ View Thesis](#-citation) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“Š Results](#-key-results) â€¢ [ğŸ“§ Contact](#-contact)

</div>

---

## ğŸ“Œ About This Research

> **Problem**: Vision Mamba models are highly vulnerable to adversarial attacks, achieving near-zero accuracy under AutoAttack despite 95%+ clean accuracy.

> **Solution**: We evaluate and compare **5 defense methods** to find the most effective robustness enhancement strategy.

> **Finding**: **Adversarial Training** is the most effective defense, while Gradient Masking and Defensive Distillation show "false robustness" that collapses under adaptive attacks.

<details>
<summary><b>ğŸ¯ Click to see Research Objectives</b></summary>

1. Evaluate baseline robustness of Vision Mamba against modern adversarial attacks
2. Implement 5 defense methods: Adversarial Training, Randomized Smoothing, Certified Robustness, Defensive Distillation, Gradient Masking
3. Compare effectiveness using AutoAttack and TRUE Adaptive Attack protocols
4. Provide recommendations for safety-critical TSR applications

</details>

---

## ğŸ”„ Research Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RESEARCH PIPELINE OVERVIEW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  PHASE 1: DATA & BASELINE                                              â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘                                                                        â•‘
  â•‘   ğŸ–¼ï¸ Dataset â”€â”€â–¶ âš™ï¸ Preprocessing â”€â”€â–¶ ğŸ§  Vision Mamba â”€â”€â–¶ ğŸ’¾ Baseline  â•‘
  â•‘   (21 classes)    (224Ã—224, Norm)      Training           Model.pth    â•‘
  â•‘                                                                        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  PHASE 2: BASELINE ATTACK EVALUATION                                   â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘                                                                        â•‘
  â•‘              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â•‘
  â•‘              â”‚   AutoAttack    â”‚    â”‚ Adaptive Attack â”‚               â•‘
  â•‘              â”‚  â€¢ APGD-CE      â”‚    â”‚  â€¢ EOT          â”‚               â•‘
  â•‘   Baseline â”€â”€â”‚  â€¢ APGD-DLR     â”‚â”€â”€â”€â”€â”‚  â€¢ BPDA         â”‚â”€â”€â–¶ ğŸ“‰ Report  â•‘
  â•‘   Model      â”‚  â€¢ FAB-T        â”‚    â”‚  â€¢ Multi-PGD    â”‚               â•‘
  â•‘              â”‚  â€¢ Square       â”‚    â”‚                 â”‚               â•‘
  â•‘              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â•‘
  â•‘                                                                        â•‘
  â•‘              Îµ âˆˆ {0.5, 1, 2, 3, 4, 5, 6, 7, 8} / 255                   â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  PHASE 3: DEFENSE TRAINING (5 Methods)                                 â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘                                                                        â•‘
  â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â•‘
  â•‘   â”‚ Adversarial  â”‚ â”‚  Randomized  â”‚ â”‚  Certified   â”‚                  â•‘
  â•‘   â”‚  Training    â”‚ â”‚  Smoothing   â”‚ â”‚   Robust     â”‚                  â•‘
  â•‘   â”‚   (PGD)      â”‚ â”‚  (Gaussian)  â”‚ â”‚   Model      â”‚                  â•‘
  â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â•‘
  â•‘                                                                        â•‘
  â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â•‘
  â•‘   â”‚  Defensive   â”‚ â”‚   Gradient   â”‚                                   â•‘
  â•‘   â”‚ Distillation â”‚ â”‚   Masking    â”‚                                   â•‘
  â•‘   â”‚  (Teacher)   â”‚ â”‚              â”‚                                   â•‘
  â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â•‘
  â•‘                         â”‚                                              â•‘
  â•‘                         â–¼                                              â•‘
  â•‘              ğŸ’¾ 5 Robust Model Variants (.pth)                         â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  PHASE 4: ROBUST MODEL EVALUATION & COMPARISON                         â•‘
  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
  â•‘                                                                        â•‘
  â•‘   Each Robust Model â”€â”€â–¶ AutoAttack â”€â”€â–¶ Adaptive Attack â”€â”€â–¶ Results    â•‘
  â•‘                                                                        â•‘
  â•‘   ğŸ“Š Compare: Clean Acc | Robust Acc | Attack Success Rate             â•‘
  â•‘   ğŸ” Detect: False Robustness (Gradient Masking, Distillation)         â•‘
  â•‘   âœ… Winner: Adversarial Training                                      â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“ Repository Structure

```
ğŸ“¦ vision-mamba-robustness/
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“‚ evaluation/                    # Attack scripts
â”‚   â”‚   â”œâ”€â”€ ğŸ”´ AutoAttack Scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ Autoattack_On_BaseModel.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Attack_on_AdversarialTrain.py
â”‚   â”‚   â”‚   â”œâ”€â”€ AutoAttack_On_Randomized_Smoothing.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Attack_On_CertifiedAccuracy.py
â”‚   â”‚   â”‚   â”œâ”€â”€ AutoAttack_on_DevensiveDestilation.py
â”‚   â”‚   â”‚   â””â”€â”€ Attack_On_Gradient_Masking_AutoAttack.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸŸ  Adaptive Attack Scripts
â”‚   â”‚       â”œâ”€â”€ Adapative_attack_on_base_Model.py
â”‚   â”‚       â”œâ”€â”€ Attack_Model_on_Adversarial_Training.py
â”‚   â”‚       â”œâ”€â”€ Attack_on_Randomized_smothing.py
â”‚   â”‚       â”œâ”€â”€ Attack_on_Gradiend_masking.py
â”‚   â”‚       â””â”€â”€ Adaptive_attack_on_Devensive_Destilation.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ training/                      # Defense training scripts
â”‚
â”œâ”€â”€ ğŸ“‚ models/                            # Model checkpoints (.pth)
â”‚   â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ adversarial_training/
â”‚   â”œâ”€â”€ randomized_smoothing/
â”‚   â”œâ”€â”€ certified_robust/
â”‚   â”œâ”€â”€ defensive_distillation/
â”‚   â””â”€â”€ gradient_masking/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                              # Dataset (see setup)
â”‚
â””â”€â”€ ğŸ“‚ results/                           # CSV reports & plots
```

---

## ğŸ“œ Evaluation Scripts

### ğŸ”´ AutoAttack Evaluation

| Script | Target Model | What it does |
|:-------|:-------------|:-------------|
| `Autoattack_On_BaseModel.py` | Baseline | Standard AutoAttack suite (APGD-CE, APGD-DLR, FAB-T, Square) |
| `Attack_on_AdversarialTrain.py` | Adversarial Training | Tests if AT model resists standard attacks |
| `AutoAttack_On_Randomized_Smoothing.py` | Randomized Smoothing | Evaluates smoothed classifier |
| `Attack_On_CertifiedAccuracy.py` | Certified Robust | Tests certified defense |
| `AutoAttack_on_DevensiveDestilation.py` | Defensive Distillation | Tests distilled model |
| `Attack_On_Gradient_Masking_AutoAttack.py` | Gradient Masking | Tests gradient obfuscation |

### ğŸŸ  Adaptive Attack Evaluation

| Script | Target Model | Special Handling |
|:-------|:-------------|:-----------------|
| `Adapative_attack_on_base_Model.py` | Baseline | TRUE adaptive PGD, multi-restart |
| `Attack_Model_on_Adversarial_Training.py` | Adversarial Training | Path resolver + adaptive eval |
| `Attack_on_Randomized_smothing.py` | Randomized Smoothing | **EOT** (Expectation over Transformation) |
| `Attack_on_Gradiend_masking.py` | Gradient Masking | **BPDA** (Backward Pass Differentiable Approximation) |
| `Adaptive_attack_on_Devensive_Destilation.py` | Defensive Distillation | Bypasses soft labels |

---

## âš™ï¸ Attack Configuration

<table>
<tr>
<td width="50%">

### AutoAttack Settings
```python
NORM = "Linf"
VERSION = "standard"
ATTACKS = [
    "apgd-ce",    # Auto-PGD + Cross Entropy
    "apgd-dlr",   # Auto-PGD + DLR loss
    "fab-t",      # Fast Adaptive Boundary
    "square"      # Score-based black-box
]

# Epsilon values (pixel scale /255)
EPS_LIST = [0.5, 1, 2, 3, 4, 5, 6, 7, 8]
```

</td>
<td width="50%">

### Adaptive Attack Settings
```python
ATTACK = "PGD"
NUM_STEPS = 100
STEP_SIZE = 2/255
NUM_RESTARTS = 10

# Defense-aware components
EOT_SAMPLES = 20      # For stochastic defenses
USE_BPDA = True       # For non-differentiable ops

# Epsilon values (pixel scale /255)
EPS_LIST = [0.5, 1, 2, 3, 4, 5, 6, 7, 8]
```

</td>
</tr>
</table>

---

## ğŸ“Š Key Results

### Performance Comparison

| Defense Method | Clean Acc. | Robust Acc.<br/>(Îµ=8/255) | Verdict |
|:---------------|:----------:|:-------------------------:|:--------|
| Baseline | 95.2% | ~0% | âŒ Highly vulnerable |
| **Adversarial Training** | 88.5% | **45.3%** | âœ… **Most effective** |
| Randomized Smoothing | 90.1% | 15.2% | âš ï¸ Partial improvement |
| Certified Robust | 85.3% | 10.8% | âš ï¸ Limited but guaranteed |
| Defensive Distillation | 92.4% | 5.1% | âŒ False robustness |
| Gradient Masking | 93.7% | 3.2% | âŒ False robustness |

### ğŸ’¡ Key Insight

> **Gradient Masking** and **Defensive Distillation** appear robust against standard attacks but **collapse under adaptive attacks**. This demonstrates "false robustness" â€” these methods only obfuscate gradients without truly improving decision boundaries.

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone & Setup

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/vision-mamba-robustness.git
cd vision-mamba-robustness

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2ï¸âƒ£ Prepare Dataset

```
data/dataset_rambu_lalu_lintas/
â”œâ”€â”€ train/          # 4,414 images
â”‚   â”œâ”€â”€ batas_kecepatan_30/
â”‚   â”œâ”€â”€ batas_kecepatan_40/
â”‚   â””â”€â”€ ... (21 classes)
â”œâ”€â”€ valid/          # 400 images
â””â”€â”€ test/           # 205 images
```

### 3ï¸âƒ£ Download Models

Place `.pth` checkpoints in `models/` folder:

```
models/
â”œâ”€â”€ baseline/best_vim_rambu_small.pth
â”œâ”€â”€ adversarial_training/best_robust_model.pth
â””â”€â”€ ...
```

### 4ï¸âƒ£ Run Evaluation

```bash
# AutoAttack on baseline
python src/evaluation/Autoattack_On_BaseModel.py

# Adaptive attack on baseline
python src/evaluation/Adapative_attack_on_base_Model.py

# AutoAttack on Adversarial Training model
python src/evaluation/Attack_on_AdversarialTrain.py
```

### 5ï¸âƒ£ View Results

Check `results/` folder for:
- ğŸ“„ `*_report.csv` â€” Accuracy metrics per epsilon
- ğŸ“Š `accuracy_vs_epsilon.png` â€” Robustness curve
- ğŸ“ˆ `asr_vs_epsilon.png` â€” Attack success rate

---

## ğŸ“‹ Requirements

```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
tqdm>=4.62.0
autoattack          # pip install git+https://github.com/fra31/auto-attack
mamba-ssm           # Vision Mamba dependencies
causal-conv1d
```

---

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@mastersthesis{solang2025visionmamba,
  author  = {Marthen Amelius Solang},
  title   = {Peningkatan Robustness Vision Mamba Terhadap Serangan Adversarial},
  school  = {Institut Teknologi Bandung},
  year    = {2025},
  type    = {Master's Thesis},
  note    = {Program Studi Magister Informatika}
}
```

---

## ğŸ”— References

| Paper | Link |
|:------|:-----|
| Vision Mamba (Zhu et al., 2024) | [arXiv](https://arxiv.org/abs/2401.09417) |
| AutoAttack (Croce & Hein, 2020) | [arXiv](https://arxiv.org/abs/2003.01690) |
| Adversarial Training (Madry et al., 2018) | [arXiv](https://arxiv.org/abs/1706.06083) |
| Randomized Smoothing (Cohen et al., 2019) | [arXiv](https://arxiv.org/abs/1902.02918) |
| BPDA Attack (Athalye et al., 2018) | [arXiv](https://arxiv.org/abs/1802.00420) |

---

## ğŸ“§ Contact

<div align="center">

**Marthen Amelius Solang**

--

*Research completed: December 2025*

</div>

---

<div align="center">

### â­ Star this repo if you find it useful!

</div>
